<h1>Models</h1>
<a target="_blank" href="https://huggingface.co/models"><h2>ðŸ¤— HuggingFace.co/models</h2></a>
<h2>Model Types</h2>
<ul>
	<li>Multimodal</li>
	<li>Computer Vision</li>
	<li>Natural Language Processing</li>
	<li>Audio</li>
	<li>Tabluar</li>
	<li>Reinforcment Learning</li>
</ul>

<h2>Parameters</h2>
<h3>
	Size does matter, but it's <a target="_blank" href="https://arxiv.org/pdf/2410.05229">debatable</a
	>
</h3>
<ul>
	<li><strong>GPT-4</strong> rumored to have 170 trillion</li>
	<li><strong>Bard</strong> 1.6 trillion</li>
	<h4>Open sourced*</h4>
	<li>Llama 3.1 1B - 420B</li>
	<h4>Sweet spot</h4>
  <li>13B - 32B</li>
</ul>

<h2>Quantisation</h2>
<ul>
	<li>It's a compression technique</li>
	<h3>
		Why e.g. <a
			target="_blank"
			href="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main"
			>Llama-3.1.8B</a
		>
	</h3>
	<li>8B parameters ~ 8GB of memory needed</li>
  <li>GPU RAM </li>
</ul>
